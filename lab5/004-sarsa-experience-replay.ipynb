{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Part I: On-policy learning and SARSA](#Part-I:-On-policy-learning-and-SARSA)\n",
    "\n",
    "[Part II: Experience replay](#Part-II:-experience-replay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part I: On-policy learning and SARSA\n",
    "\n",
    "_This notebook builds upon `003-q-learning.ipynb`, or to be exact your implementation of QLearningAgent._\n",
    "\n",
    "The policy we're gonna use is epsilon-greedy policy, where agent takes optimal action with probability $(1-\\epsilon)$, otherwise samples action at random. Note that agent __can__ occasionally sample optimal action during random sampling by pure chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can copy your `QLearningAgent` implementation from previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "\n",
    "        Note: please avoid using self._qValues directly.\n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    #---------------------START OF YOUR CODE---------------------#\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Compute your agent's estimate of V(s) using current q-values\n",
    "        V(s) = max_over_action Q(state,action) over possible actions.\n",
    "        Note: please take into account that q-values can be negative.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return max(map(lambda action: self.get_qvalue(state, action), possible_actions))\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here:\n",
    "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "\n",
    "        q = (1 - learning_rate) * self.get_qvalue(state, action) + learning_rate * (\n",
    "                reward + gamma * self.get_value(next_state))\n",
    "\n",
    "        self.set_qvalue(state, action, q)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values).\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        action_i = np.argmax(map(lambda action: self.get_qvalue(state, action), possible_actions))\n",
    "\n",
    "        return possible_actions[action_i]\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.\n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list).\n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(possible_actions)\n",
    "\n",
    "        return self.get_best_action(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we gonna implement Expected Value SARSA on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EVSarsaAgent(QLearningAgent):\n",
    "    \"\"\" \n",
    "    An agent that changes some of q-learning functions to implement Expected Value SARSA. \n",
    "    Note: this demo assumes that your implementation of QLearningAgent.update uses get_value(next_state).\n",
    "    If it doesn't, please add\n",
    "        def update(self, state, action, reward, next_state):\n",
    "            and implement it for Expected Value SARSA's V(s')\n",
    "    \"\"\"\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\" \n",
    "        Returns Vpi for current state under epsilon-greedy policy:\n",
    "          V_{pi}(s) = sum _{over a_i} {pi(a_i | s) * Q(s, a_i)}\n",
    "\n",
    "        Hint: all other methods from QLearningAgent are still accessible.\n",
    "        \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        best_action = self.get_best_action(state)\n",
    "        # q_max = self.get_value(state)\n",
    "\n",
    "        res = 0\n",
    "        for action in possible_actions:\n",
    "            if action == best_action:\n",
    "                # epsilon/l + 1 - epsilon =\n",
    "                # (1 - l)epsilon / l + 1 =\n",
    "                # 1 - (l - 1)*epsilon/l\n",
    "                res += (1 - (len(possible_actions) - 1)*epsilon) * self.get_qvalue(state, action)\n",
    "            else:\n",
    "                # epsilon / l\n",
    "\n",
    "                res += (epsilon / len(possible_actions) * self.get_qvalue(state, action))\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cliff World\n",
    "\n",
    "Let's now see how our algorithm compares against q-learning in case where we force agent to explore all the time.\n",
    "\n",
    "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/cliffworld.png width=600>\n",
    "<center><i>image by cs188</i></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This is a simple implementation of the Gridworld Cliff\n",
      "    reinforcement learning task.\n",
      "\n",
      "    Adapted from Example 6.6 (page 106) from [Reinforcement Learning: An Introduction\n",
      "    by Sutton and Barto](http://incompleteideas.net/book/bookdraft2018jan1.pdf).\n",
      "\n",
      "    With inspiration from:\n",
      "    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
      "\n",
      "    ### Description\n",
      "    The board is a 4x12 matrix, with (using NumPy matrix indexing):\n",
      "    - [3, 0] as the start at bottom-left\n",
      "    - [3, 11] as the goal at bottom-right\n",
      "    - [3, 1..10] as the cliff at bottom-center\n",
      "\n",
      "    If the agent steps on the cliff it returns to the start.\n",
      "    An episode terminates when the agent reaches the goal.\n",
      "\n",
      "    ### Actions\n",
      "    There are 4 discrete deterministic actions:\n",
      "    - 0: move up\n",
      "    - 1: move right\n",
      "    - 2: move down\n",
      "    - 3: move left\n",
      "\n",
      "    ### Observations\n",
      "    There are 3x12 + 1 possible states. In fact, the agent cannot be at the cliff, nor at the goal\n",
      "    (as this results the end of episode). They remain all the positions of the first 3 rows plus the bottom-left cell.\n",
      "    The observation is simply the current position encoded as\n",
      "    [flattened index](https://numpy.org/doc/stable/reference/generated/numpy.unravel_index.html).\n",
      "\n",
      "    ### Reward\n",
      "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward.\n",
      "\n",
      "    ### Arguments\n",
      "\n",
      "    ```\n",
      "    gym.make('CliffWalking-v0')\n",
      "    ```\n",
      "\n",
      "    ### Version History\n",
      "    - v0: Initial version release\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.envs.toy_text\n",
    "\n",
    "env = gym.envs.toy_text.CliffWalkingEnv()\n",
    "n_actions = env.action_space.n\n",
    "env.reset()\n",
    "print(env.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our cliffworld has one difference from what's on the image: there is no wall.\n",
    "# Agent can choose to go as close to the cliff as it wishes. x:start, T:exit, C:cliff, o: flat ground\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10 ** 4):\n",
    "    \"\"\"This function should \n",
    "    - run a full game, actions given by agent.get_action(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        agent.update(s, a, r, next_s)\n",
    "\n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent_sarsa = EVSarsaAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
    "                           get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
    "                          get_legal_actions=lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2000\n",
      "EVSARSA mean reward = -9509.74\n",
      "QLEARNING mean reward = -9500.46\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEICAYAAAC3Y/QeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcUElEQVR4nO3de5QU9Z338feXEWeioAgiclvBR8RwcZQZGM+ZaEAU0CiIlwXPRiHZgDmKLicG0UCUVVgNZncfE40ejBxETQbFC4gYggR21QSVAURAhEHgkZsXQGSiyO37/NEF6RmmZ+jpG/D7vM7pM9W/+lXVt6t7Pl1dXVVt7o6IiISlQa4LEBGR7FP4i4gESOEvIhIghb+ISIAU/iIiAVL4i4gESOEvwTKzX5jZ76PhdmbmZnZCrusSyQaFvwTL3f/D3X+S6zoSMbMLzKzczL6O/l6QoF++mT1lZhvMbJeZLTWzK7JcrhxjFP4iRyEzOxGYATwLnAY8DcyI2qs7AfgE+D5wKjAWeN7M2mWnWjkWKfzlmGBmrczsRTP73MzWmdkdcePGmdl0M5sWbfkuNrPCuPGjzWxTNO4jM+sdN92ztSxvppltN7MKMxtWbXnPm9nUaJ4rzKw4zQ+5J7FQ/7/u/q27/wYw4NLqHd397+4+zt3Xu/sBd58FrAOK0lyTHEcU/nLUM7MGwKvA+0BroDcw0sz6xnUbALwANAX+ALxiZg3NrCMwAuju7o2BvsD6I1hsGbARaAVcD/yHmcUHb/+oTxNgJvBoLfUvM7MvE9x+l2CyzsAyr3r9lWVRe63MrAVwLrCirr4SLoW/HAu6A83d/X533+PuHwNPAoPj+pS7+3R33wv8F1AAXATsB/KBTmbWMNo6XlvbwsysLVAKjHb33e6+FPg9cHNct7fcfba77weeAQoPn1OMu5/v7k0S3G5NMFkjYGe1tp1A4zpqbwg8Bzzt7qtq6ythU/jLseAsoFX8FjPwC6BFXJ9PDg64+wGirXZ3rwBGAuOAz8yszMxa1bG8VsB2d98V17aB2KeOg7bGDX8NFKT5SKFK4JRqbacAu2roCxz6hPQMsIfYpx2RhBT+ciz4BFhXbYu5sbtfGden7cGBKATbAJsB3P0P7v49Ym8iDvyqjuVtBpqaWfxW9j8Bm+pTfPSdQGWC2xMJJlsBnG9mFtd2Pgl25UT9niL2hnhd9AlIJCGFvxwL3gV2RV/cfsfM8sysi5l1j+tTZGbXRlvfI4FvgYVm1tHMLjWzfGA38A1woLaFufsnwF+BB82swMzOB/6V2JE3SXP3zu7eKMHtpwkmW0Bsl9Ud0aGcB7fk/5Kg/+PAd4Gr3f2b+tQpYVH4y1Ev2q9+FXABsaNYviC2D/7UuG4zgEHADuAm4Npo6zcfeCiaZitwBnDPESz2RqAdsU8BLwP3ufsbqT+aI+Pue4BriH3P8CXwY+CaqP3gCWqvR8NnAbcQWz9b4z5V/Eu26pVjj+nHXORYZ2bjgHPc/Ye5rkXkWKEtfxGRAOUs/M2sX3TCTYWZ3Z2rOkREQpST3T5mlgesBi4ndkjee8CN7r4y68WIiAQoV1v+PYAKd/84+gKrjNgZmiIikgW5unxta+JOyiG29V8S38HMhgPDAb7zne8UtW3blvo6cOAADRocfV9vqK7kqK7kqK7kHI91rV69+gt3b17jSHfP+o3YtVJ+H3f/JuDRRP2Lioo8FfPnz09p+kxRXclRXclRXck5HusCFnmCXM3V29wm4s7IJHY2Zr3OnhQRkeTlKvzfAzqYWfvo+uSDiV0ZUUREsiAn+/zdfV90uvocIA+Y7O66/KyISJbk7PdK3X02MDtXyxeR3Nu7dy8bN25k9+7duS6FU089lQ8//DDXZRzmSOoqKCigTZs2NGzY8Ijnqx+rFpGc2bhxI40bN6Zdu3ZUvYBp9u3atYvGjWv9uYScqKsud2fbtm1s3LiR9u3bH/F8j77jmkQkGLt376ZZs2Y5D/5jmZnRrFmzpD89KfxFJKcU/KmrzzpU+IuIBEjhLyISIIW/iEgO7Nu3L6fLV/iLSNCeffZZevToQWlpKbfccguPPfYYo0aNOjR+ypQpjBgxosZp//73v/ODH/yAwsJCunTpwrRp0wC4//776d69O126dGH48OEHL2NDz549GTlyJMXFxTzyyCO88MILdOnShcLCQi655BIA1q9fz8UXX0y3bt3o1q0b77zzTkYetw71FJGjwr+/uoKVm79K6zw7tTqF+67unHD8hx9+yLRp03j77bfZvXs3o0ePplGjRrz88ss8/PDDAEybNo0xY8bUOP2f/vQnWrVqxWuvvQbAzp07ARgxYgT33nsvADfddBOzZs3i6quvBmDPnj0sWrQIgK5duzJnzhxat27Nl19+CcAZZ5zB3LlzKSgoYM2aNQwaNIjFixenvjKq0Za/iARr3rx5lJeX0717d0pLS5k3bx7r1q3j7LPPZuHChWzbto1Vq1ZRWlpa4/Rdu3Zl7ty5jB49mjfffJNTT439rPT8+fMpKSmha9eu/OUvf2HFin9cwGDQoEGHhktLSxk6dChPPvkk+/fvB2Invg0bNoyuXbtyww03sGrVqow8dm35i8hRobYt9Exxd4YMGcKDDz5Y5WSqyZMn8/zzz3PeeecxcODAhIdSnnvuuSxevJjZs2czduxYevfuzV133cWtt97KokWLaNu2LePGjatyDP7JJ598aPiJJ57gnXfe4bXXXqOoqIjy8nJ++9vf0qJFC95//30OHDhAQUFBRh67tvxFJFi9e/dm+vTpfPbZZwBs376dDRs2MHDgQGbMmMEf//hHBg8enHD6zZs3c9JJJ/HDH/6QUaNGsXjx4kNBf/rpp1NZWcn06dMTTr927VpKSkq4//77ad68OZ988gk7d+6kZcuWNGjQgGeeeebQJ4J005a/iASrU6dOjB8/nj59+rBv3z7y8/N57LHHOOuss/jud7/LypUr6dGjR8LpP/jgA0aNGkWDBg1o2LAhjz/+OE2aNGHYsGF06dKFM888k+7duyecftSoUaxZswZ3p3fv3hQWFnLrrbdy3XXXMXXqVPr161flk0I6KfxFJGiDBg1i0KBBh11DZ9asWXVO27dvX/r27XtY+/jx4xk/fvxh7QsWLKhy/6WXXjqsT4cOHVi2bNmh+2PHjq2zjvrQbh8RkQBpy19EpA7btm2jd+/eh7XPmzePZs2a5aCi1Cn8RUTq0KxZM5YuXZrrMtJKu31ERAKk8BcRCZDCX0QkQAp/EZEAKfxFRKoZOnRorWfmpsPmzZu5/vrrM7qM2ij8RUQypLZr9rdq1SrjbzC10aGeIhK0CRMm8PTTT9OsWTPatWtHUVFRlfHl5eX87Gc/o7KyktNPP50pU6bQsmVLnnzySSZNmsSePXs455xzeOaZZzjppJMYOnQoBQUFLFmyhNLSUrZv384pp5zCokWL2Lp1KxMnTuT6669n/fr1XHXVVSxfvpwpU6Ywc+ZMvv76a9auXcvAgQOZOHEiAFOnTuWRRx6hSZMmFBYWkp+fz6OPPpry41b4i8jR4fW7YesH6Z3nmV3hiocSji4vL6esrIylS5eyY8cOvv/971cJ/71793L77bczY8YMmjdvfuja/pMnT+baa69l2LBhQOwSDE899RS33347ABs3buSvf/0reXl5DB06lC1btvDWW2+xatUq+vfvX+PunqVLl7JkyRLy8/Pp2LEjt99+O3l5eUycOJElS5bQuHFjLr30UgoLC9OyahT+IhKsN998k4EDB3LSSSexf/9++vfvX2X8Rx99xPLly7n88ssB2L9/Py1btgRg+fLljB07li+//JLKysoq1/i54YYbyMvLO3T/mmuuoUGDBnTq1IlPP/20xlp69+596PcAOnXqxIYNG/jiiy8oLS2ladOmh+a7evXqtDx2hb+IHB1q2ULPFXenc+fO/O1vfzts3NChQ3nllVcoLCxkypQpVS7aVv1KnPn5+VXmWZP4Pnl5eRn/jV994Ssiwbrkkkt45ZVX+Oabb9i1axevvvpqlfEdO3bk888/PxT+e/fuPfSrXLt27aJly5bs3buX5557LiP1de/enbfffpsdO3awb98+XnzxxbTNW1v+IhKsbt26MWjQIAoLC2nWrNlh194/8cQTmT59OnfccQc7d+5k3759jBw5ks6dO/PAAw9QUlJC8+bNKSkpYdeuXWmvr3Xr1tx555306NGDpk2bct555x3aNZQydz/qb0VFRZ6K+fPnpzR9pqiu5Kiu5BwLda1cuTJ3hVTz1Vdf+X333ecPP/xwrkupYvPmze7uvnfvXr/qqqv8pZdeqrFfTesSWOQJclW7fUREjmIPPvggF1xwAV26dKF9+/Zcc801aZmvdvuIiETGjRuX6xIOM2HChCq/MJYu2vIXkZzyBEe/yJGrzzpU+ItIzhQUFLBt2za9AaTA3dm2bRsFBQVJTafdPiKSM23atGHjxo18/vnnuS6F3bt3Jx2g2XAkdRUUFNCmTZuk5qvwF5GcadiwIe3bt891GQAsWLCACy+8MNdlHCZTdaW028fMbjCzFWZ2wMyKq427x8wqzOwjM+sb194vaqsws7tTWb6IiNRPqvv8lwPXAv8b32hmnYDBQGegH/A7M8szszzgMeAKoBNwY9RXRESyKKXdPu7+IYCZVR81AChz92+BdWZWAfSIxlW4+8fRdGVR35Wp1CEiIsmxdHzLbmYLgJ+7+6Lo/qPAQnd/Nrr/FPB61L2fu/8kar8JKHH3ETXMczgwHKBFixZFZWVl9a6vsrKSRo0a1Xv6TFFdyVFdyVFdyTke6+rVq1e5uxfXNK7OLX8zewM4s4ZRY9x9Rr0qOgLuPgmYBFBcXOw9e/as97wWLFhAKtNniupKjupKjupKTmh11Rn+7n5ZPea7CWgbd79N1EYt7SIikiWZOslrJjDYzPLNrD3QAXgXeA/oYGbtzexEYl8Kz8xQDSIikkBKX/ia2UDgt0Bz4DUzW+rufd19hZk9T+yL3H3Abe6+P5pmBDAHyAMmu/uKlB6BiIgkLdWjfV4GXk4wbgIwoYb22cDsVJYrIiKp0bV9REQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQlQSuFvZg+b2SozW2ZmL5tZk7hx95hZhZl9ZGZ949r7RW0VZnZ3KssXEZH6SXXLfy7Qxd3PB1YD9wCYWSdgMNAZ6Af8zszyzCwPeAy4AugE3Bj1FRGRLEop/N39z+6+L7q7EGgTDQ8Aytz9W3dfB1QAPaJbhbt/7O57gLKor4iIZJG5e3pmZPYqMM3dnzWzR4GF7v5sNO4p4PWoaz93/0nUfhNQ4u4japjfcGA4QIsWLYrKysrqXVtlZSWNGjWq9/SZorqSo7qSo7qSczzW1atXr3J3L65p3Al1TWxmbwBn1jBqjLvPiPqMAfYBz9Wrwhq4+yRgEkBxcbH37Nmz3vNasGABqUyfKaorOaorOaorOaHVVWf4u/tltY03s6HAVUBv/8fHiE1A27hubaI2amkXEZEsSfVon37AXUB/d/86btRMYLCZ5ZtZe6AD8C7wHtDBzNqb2YnEvhSemUoNIiKSvDq3/OvwKJAPzDUziO3n/6m7rzCz54GVxHYH3ebu+wHMbAQwB8gDJrv7ihRrEBGRJKUU/u5+Ti3jJgATamifDcxOZbkiIpIaneErIhIghb+ISIAU/iIiAVL4i4gESOEvIhIghb+ISIAU/iIiAVL4i4gESOEvIhIghb+ISIAU/iIiAVL4i4gESOEvIhIghb+ISIAU/iIiAVL4i4gESOEvIhIghb+ISIAU/iIiAVL4i4gESOEvIhIghb+ISIAU/iIiAVL4i4gESOEvIhIghb+ISIAU/iIiAVL4i4gESOEvIhIghb+ISIAU/iIiAVL4i4gESOEvIhIghb+ISIAU/iIiAUop/M3sATNbZmZLzezPZtYqajcz+42ZVUTju8VNM8TM1kS3Iak+ABERSV6qW/4Pu/v57n4BMAu4N2q/AugQ3YYDjwOYWVPgPqAE6AHcZ2anpViDiIgkKaXwd/ev4u6eDHg0PACY6jELgSZm1hLoC8x19+3uvgOYC/RLpQYREUmeuXvdvWqbgdkE4GZgJ9DL3T83s1nAQ+7+VtRnHjAa6AkUuPv4qP2XwDfu/usa5juc2KcGWrRoUVRWVlbvGisrK2nUqFG9p88U1ZUc1ZUc1ZWc47GuXr16lbt7cY0j3b3WG/AGsLyG24Bq/e4B/j0angV8L27cPKAY+DkwNq79l8DP66qhqKjIUzF//vyUps8U1ZUc1ZUc1ZWc47EuYJEnyNUT6nrncPfLjvBN5jlgNrF9+puAtnHj2kRtm4ht/ce3LzjC+YuISJqkerRPh7i7A4BV0fBM4OboqJ+LgJ3uvgWYA/Qxs9OiL3r7RG0iIpJFdW751+EhM+sIHAA2AD+N2mcDVwIVwNfAjwDcfbuZPQC8F/W73923p1iDiIgkKaXwd/frErQ7cFuCcZOByaksV0REUqMzfEVEAqTwFxEJkMJfRCRACn8RkQAp/EVEAqTwFxEJkMJfRCRACn8RkQAp/EVEAqTwFxEJkMJfRCRACn8RkQAp/EVEAqTwFxEJkMJfRCRACn8RkQAp/EVEAqTwFxEJkMJfRCRACn8RkQAp/EVEAqTwFxEJkMJfRCRACn8RkQAp/EVEAqTwFxEJkMJfRCRACn8RkQAp/EVEAqTwFxEJkMJfRCRACn8RkQAp/EVEAqTwFxEJUFrC38zuNDM3s9Oj+2ZmvzGzCjNbZmbd4voOMbM10W1IOpYvIiLJOSHVGZhZW6AP8P/imq8AOkS3EuBxoMTMmgL3AcWAA+VmNtPdd6Rah4iIHLl0bPn/N3AXsTA/aAAw1WMWAk3MrCXQF5jr7tujwJ8L9EtDDSIikgRz97p7JZrYbABwqbv/m5mtB4rd/QszmwU85O5vRf3mAaOBnkCBu4+P2n8JfOPuv65h3sOB4QAtWrQoKisrq3edlZWVNGrUqN7TZ4rqSo7qSo7qSs7xWFevXr3K3b24pnF17vYxszeAM2sYNQb4BbFdPmnn7pOASQDFxcXes2fPes9rwYIFpDJ9pqiu5Kiu5Kiu5IRWV53h7+6X1dRuZl2B9sD7ZgbQBlhsZj2ATUDbuO5torZNxLb+49sX1KNuERFJQb33+bv7B+5+hru3c/d2wEagm7tvBWYCN0dH/VwE7HT3LcAcoI+ZnWZmpxH71DAn9YchIiLJSPlonwRmA1cCFcDXwI8A3H27mT0AvBf1u9/dt2eoBhERSSBt4R9t/R8cduC2BP0mA5PTtVwREUmezvAVEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRACn8RUQCpPAXEQmQwl9EJEAKfxGRAKUU/mY2zsw2mdnS6HZl3Lh7zKzCzD4ys75x7f2itgozuzuV5YuISP2ckIZ5/Le7/zq+wcw6AYOBzkAr4A0zOzca/RhwObAReM/MZrr7yjTUISIiRygd4V+TAUCZu38LrDOzCqBHNK7C3T8GMLOyqK/CX0Qki9IR/iPM7GZgEXCnu+8AWgML4/psjNoAPqnWXlLTTM1sODA8ultpZh+lUOPpwBcpTJ8pqis5qis5qis5x2NdZyUaUWf4m9kbwJk1jBoDPA48AHj09z+BH9evxqrcfRIwKR3zMrNF7l6cjnmlk+pKjupKjupKTmh11Rn+7n7ZkczIzJ4EZkV3NwFt40a3idqopV1ERLIk1aN9WsbdHQgsj4ZnAoPNLN/M2gMdgHeB94AOZtbezE4k9qXwzFRqEBGR5KW6z3+imV1AbLfPeuAWAHdfYWbPE/sidx9wm7vvBzCzEcAcIA+Y7O4rUqzhSKRl91EGqK7kqK7kqK7kBFWXuXsm5isiIkcxneErIhIghb+ISICO6/DP5aUkzKytmc03s5VmtsLM/i1qT/qSGBmobb2ZfRAtf1HU1tTM5prZmujvaVG7mdlvorqWmVm3DNXUMW6dLDWzr8xsZC7Wl5lNNrPPzGx5XFvS68fMhkT915jZkAzV9bCZrYqW/bKZNYna25nZN3Hr7Ym4aYqi578iqt0yUFfOL/2SoK5pcTWtN7OlUXs211eibMjua8zdj8sbsS+U1wJnAycC7wOdsrj8lkC3aLgxsBroBIwDfl5D/05RjflA+6j2vAzVth44vVrbRODuaPhu4FfR8JXA64ABFwHvZOm520rsBJWsry/gEqAbsLy+6wdoCnwc/T0tGj4tA3X1AU6Ihn8VV1e7+H7V5vNuVKtFtV+RgbqSet4y8f9aU13Vxv8ncG8O1leibMjqa+x43vLvQXQpCXffAxy8lERWuPsWd18cDe8CPuQfZznX5NAlMdx9HRB/SYxsGAA8HQ0/DVwT1z7VYxYCTazqIb6Z0BtY6+4baumTsfXl7v8LbK9hecmsn77AXHff7rGz3ucC/dJdl7v/2d33RXcXEjt3JqGotlPcfaHHEmRq3GNJW121SPS8pf3/tba6oq33fwb+WNs8MrS+EmVDVl9jx3P4t+bwS0nUFr4ZY2btgAuBd6KmEdHHt8kHP9qR3Xod+LOZlVvsMhoALdx9SzS8FWiRg7oOGkzVf8pcry9Ifv3kYr39mNgW4kHtzWyJmf2PmV0ctbWOaslGXck8b9leXxcDn7r7mri2rK+vatmQ1dfY8Rz+RwUzawS8CIx096+IXRLj/wAXAFuIffTMtu+5ezfgCuA2M7skfmS0hZOTY4AtdvJff+CFqOloWF9V5HL9JGJmY4idU/Nc1LQF+Cd3vxD4GfAHMzsliyUddc9bNTdSdQMj6+urhmw4JBuvseM5/Gu7xERWmFlDYk/uc+7+EoC7f+ru+939APAk/9hVkbV63X1T9Pcz4OWohk8P7s6J/n6W7boiVwCL3f3TqMacr69Isusna/WZ2VDgKuBfotAg2q2yLRouJ7Y//dyohvhdQxmpqx7PWzbX1wnAtcC0uHqzur5qygay/Bo7nsM/p5eSiPYpPgV86O7/Fdee7CUx0l3XyWbW+OAwsS8Ml0fLP3i0wBBgRlxdN0dHHFwE7Iz7aJoJVbbIcr2+4iS7fuYAfczstGiXR5+oLa3MrB9wF9Df3b+Oa29uZnnR8NnE1s/HUW1fmdlF0Wv05rjHks66juZLv1wGrHL3Q7tzsrm+EmUD2X6NpfKt9dF+I/Yt+Wpi7+Jjsrzs7xH72LYMWBrdrgSeAT6I2mcCLeOmGRPV+hEpHlFQS11nEzuS4n1gxcH1AjQD5gFrgDeAplG7EfsBnrVR3cUZXGcnA9uAU+Pasr6+iL35bAH2EtuP+q/1WT/E9sFXRLcfZaiuCmL7fQ++xp6I+l4XPb9LgcXA1XHzKSYWxmuBR4nO9E9zXUk/b+n+f62prqh9CvDTan2zub4SZUNWX2O6vIOISICO590+IiKSgMJfRCRACn8RkQAp/EVEAqTwFxEJkMJfRCRACn8RkQD9fx+iNCv4Il/1AAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8064/3807240511.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m5000\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     \u001B[0mrewards_sarsa\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mplay_and_train\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent_sarsa\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m     \u001B[0mrewards_ql\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mplay_and_train\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent_ql\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[1;31m# Note: agent.epsilon stays constant\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8064/392273808.py\u001B[0m in \u001B[0;36mplay_and_train\u001B[1;34m(env, agent, t_max)\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[0ma\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_action\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m         \u001B[0mnext_s\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m         \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnext_s\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\projects\\python\\datavisualization\\venv\\lib\\site-packages\\gym\\envs\\toy_text\\cliffwalking.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, a)\u001B[0m\n\u001B[0;32m    122\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    123\u001B[0m         \u001B[0mtransitions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mP\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 124\u001B[1;33m         \u001B[0mi\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcategorical_sample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtransitions\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnp_random\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    125\u001B[0m         \u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0md\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtransitions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    126\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\projects\\python\\datavisualization\\venv\\lib\\site-packages\\gym\\envs\\toy_text\\utils.py\u001B[0m in \u001B[0;36mcategorical_sample\u001B[1;34m(prob_n, np_random)\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[0mprob_n\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprob_n\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[0mcsprob_n\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcumsum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprob_n\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcsprob_n\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mnp_random\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36margmax\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;32mc:\\projects\\python\\datavisualization\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001B[0m in \u001B[0;36margmax\u001B[1;34m(a, axis, out)\u001B[0m\n\u001B[0;32m   1193\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1194\u001B[0m     \"\"\"\n\u001B[1;32m-> 1195\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_wrapfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'argmax'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1196\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1197\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\projects\\python\\datavisualization\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001B[0m in \u001B[0;36m_wrapfunc\u001B[1;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     56\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 57\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mbound\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     58\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m         \u001B[1;31m# A TypeError occurs if the object does have such a method in its\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def moving_average(x, span=100):\n",
    "    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "\n",
    "rewards_sarsa, rewards_ql = [], []\n",
    "\n",
    "for i in range(5000):\n",
    "    rewards_sarsa.append(play_and_train(env, agent_sarsa))\n",
    "    rewards_ql.append(play_and_train(env, agent_ql))\n",
    "    # Note: agent.epsilon stays constant\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print('I:', i)\n",
    "        print('EVSARSA mean reward =', np.mean(rewards_sarsa[-100:]))\n",
    "        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
    "        plt.title(\"epsilon = %s\" % agent_ql.epsilon)\n",
    "        plt.plot(moving_average(rewards_sarsa), label='ev_sarsa')\n",
    "        plt.plot(moving_average(rewards_ql), label='qlearning')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.ylim(None, 0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now see what did the algorithms learn by visualizing their actions at every state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def draw_policy(env, agent):\n",
    "    \"\"\" Prints CliffWalkingEnv policy with arrows. Hard-coded. \"\"\"\n",
    "    n_rows, n_cols = env._cliff.shape\n",
    "\n",
    "    actions = '^>v<'\n",
    "\n",
    "    for yi in range(n_rows):\n",
    "        for xi in range(n_cols):\n",
    "            if env._cliff[yi, xi]:\n",
    "                print(\" C \", end='')\n",
    "            elif (yi * n_cols + xi) == env.start_state_index:\n",
    "                print(\" X \", end='')\n",
    "            elif (yi * n_cols + xi) == n_rows * n_cols - 1:\n",
    "                print(\" T \", end='')\n",
    "            else:\n",
    "                print(\" %s \" %\n",
    "                      actions[agent.get_best_action(yi * n_cols + xi)], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning\n",
      " ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ \n",
      " ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ \n",
      " ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ \n",
      " X  C  C  C  C  C  C  C  C  C  C  T \n",
      "SARSA\n",
      " ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ \n",
      " ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ \n",
      " ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^ \n",
      " X  C  C  C  C  C  C  C  C  C  C  T \n"
     ]
    }
   ],
   "source": [
    "print(\"Q-Learning\")\n",
    "draw_policy(env, agent_ql)\n",
    "\n",
    "print(\"SARSA\")\n",
    "draw_policy(env, agent_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### More on SARSA\n",
    "\n",
    "Here are some of the things you can do if you feel like it:\n",
    "\n",
    "* Play with epsilon. See learned how policies change if you set epsilon to higher/lower values (e.g. 0.75).\n",
    "* Expected Value SARSA for softmax policy __(2pts)__:\n",
    "$$ \\pi(a_i \\mid s) = \\operatorname{softmax} \\left( \\left\\{ {Q(s, a_j) \\over \\tau} \\right\\}_{j=1}^n \\right)_i = {\\operatorname{exp} \\left( Q(s,a_i) / \\tau \\right)  \\over {\\sum_{j}  \\operatorname{exp} \\left( Q(s,a_j) / \\tau  \\right)}} $$\n",
    "* Implement N-step algorithms and TD($\\lambda$): see [Sutton's book](http://incompleteideas.net/book/RLbook2020.pdf) chapter 7 and chapter 12.\n",
    "* Use those algorithms to train on CartPole in previous / next assignment for this week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part II: Experience Replay\n",
    "\n",
    "There's a powerful technique that you can use to improve sample efficiency for off-policy algorithms: [spoiler] Experience replay :)\n",
    "\n",
    "The catch is that you can train Q-learning and EV-SARSA on `<s,a,r,s'>` tuples even if they aren't sampled under current agent's policy. So here's what we're gonna do:\n",
    "\n",
    "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png width=480>\n",
    "\n",
    "#### Training with experience replay\n",
    "1. Play game, sample `<s,a,r,s'>`.\n",
    "2. Update q-values based on `<s,a,r,s'>`.\n",
    "3. Store `<s,a,r,s'>` transition in a buffer. \n",
    " 3. If buffer is full, delete earliest data.\n",
    "4. Sample K such transitions from that buffer and update q-values based on them.\n",
    "\n",
    "\n",
    "To enable such training, first we must implement a memory structure that would act like such a buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "\n",
    "        Note: for this assignment you can pick any data structure you want.\n",
    "              If you want to keep it simple, you can store a list of tuples of (s, a, r, s') in self._storage\n",
    "              However you may find out there are faster and/or more memory-efficient ways to do so.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "\n",
    "        # OPTIONAL: YOUR CODE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Make sure, _storage will not exceed _maxsize. \n",
    "        Make sure, FIFO rule is being followed: the oldest examples has to be removed earlier\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        # add data to storage\n",
    "        self._storage.insert(0, data)\n",
    "        self._storage = self._storage[:self._maxsize]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = np.random.choice(range(len(self._storage)), batch_size)\n",
    "\n",
    "        # collect <s,a,r,s',done> for each index\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        is_done = []\n",
    "\n",
    "        for i in idxes:\n",
    "            obs_t, action, reward, obs_tp1, done = self._storage[i]\n",
    "            states += [obs_t]\n",
    "            actions += [action]\n",
    "            rewards += [reward]\n",
    "            next_states += [obs_tp1]\n",
    "            is_done += [done]\n",
    "\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(is_done),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Some tests to make sure your buffer works right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "def obj2arrays(obj):\n",
    "    for x in obj:\n",
    "        yield np.array([x])\n",
    "\n",
    "\n",
    "def obj2sampled(obj):\n",
    "    return tuple(obj2arrays(obj))\n",
    "\n",
    "\n",
    "replay = ReplayBuffer(2)\n",
    "obj1 = (0, 1, 2, 3, True)\n",
    "obj2 = (4, 5, 6, 7, False)\n",
    "replay.add(*obj1)\n",
    "assert replay.sample(1) == obj2sampled(obj1),\\\n",
    "    \"If there's just one object in buffer, it must be retrieved by buf.sample(1)\"\n",
    "replay.add(*obj2)\n",
    "assert len(replay) == 2, \"Please make sure __len__ methods works as intended.\"\n",
    "replay.add(*obj2)\n",
    "assert len(replay) == 2, \"When buffer is at max capacity, replace objects instead of adding new ones.\"\n",
    "assert tuple(np.unique(a) for a in replay.sample(100)) == obj2sampled(obj2)\n",
    "replay.add(*obj1)\n",
    "assert max(len(np.unique(a)) for a in replay.sample(100)) == 2\n",
    "replay.add(*obj1)\n",
    "assert tuple(np.unique(a) for a in replay.sample(100)) == obj2sampled(obj1)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's use this buffer to improve training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def play_and_train_with_replay(env, agent, replay=None,\n",
    "                               t_max=10 ** 4, replay_batch_size=32):\n",
    "    \"\"\"\n",
    "    This function should \n",
    "    - run a full game, actions given by agent.get_action(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\n",
    "    :param replay: ReplayBuffer where agent can store and sample (s,a,r,s',done) tuples.\n",
    "        If None, do not use experience replay\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # get agent to pick action given state s\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "\n",
    "        # update agent on current transition. Use agent.update\n",
    "        agent.update(s, a, r, next_s)\n",
    "\n",
    "        if replay is not None:\n",
    "            # store current <s,a,r,s'> transition in buffer\n",
    "            replay.add(s, a, r, next_s, done)\n",
    "\n",
    "            # sample replay_batch_size random transitions from replay,\n",
    "            # then update agent on each of them in a loop\n",
    "            s_, a_, r_, next_s_, done_ = replay.sample(replay_batch_size)\n",
    "            for i in range(replay_batch_size):\n",
    "                agent.update(s_[i], a_[i], r_[i], next_s_[i])\n",
    "\n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create two agents: first will use experience replay, second will not.\n",
    "\n",
    "agent_baseline = QLearningAgent(\n",
    "    alpha=0.5, epsilon=0.25, discount=0.99,\n",
    "    get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "agent_replay = QLearningAgent(\n",
    "    alpha=0.5, epsilon=0.25, discount=0.99,\n",
    "    get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "replay = ReplayBuffer(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline : eps = 2.9191091959171894e-05 mean reward = -200.0\n",
      "ExpReplay: eps = 2.9191091959171894e-05 mean reward = -200.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def moving_average(x, span=100):\n",
    "    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "\n",
    "rewards_replay, rewards_baseline = [], []\n",
    "\n",
    "for i in range(1000):\n",
    "    rewards_replay.append(\n",
    "        play_and_train_with_replay(env, agent_replay, replay))\n",
    "    rewards_baseline.append(\n",
    "        play_and_train_with_replay(env, agent_baseline, replay=None))\n",
    "\n",
    "    agent_replay.epsilon *= 0.99\n",
    "    agent_baseline.epsilon *= 0.99\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print('Baseline : eps =', agent_replay.epsilon,\n",
    "              'mean reward =', np.mean(rewards_baseline[-10:]))\n",
    "        print('ExpReplay: eps =', agent_baseline.epsilon,\n",
    "              'mean reward =', np.mean(rewards_replay[-10:]))\n",
    "        plt.plot(moving_average(rewards_replay), label='exp. replay')\n",
    "        plt.plot(moving_average(rewards_baseline), label='baseline')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### What to expect:\n",
    "\n",
    "Experience replay, if implemented correctly, will improve algorithm's initial convergence a lot, but it shouldn't affect the final performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}